{\rtf1\fbidis\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset0 Consolas;}}
{\colortbl ;\red255\green0\blue0;\red255\green255\blue0;\red106\green135\blue89;\red204\green120\blue50;\red169\green183\blue198;\red98\green151\blue85;}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\ltrpar\sa200\sl276\slmult1\b\f0\fs36\lang9 README \b0\fs22\par
\b Deep learning final project - project_307923052_302636675\par
\b0 1. Downlad the code from the git site. \par
2. \ul Download Dataset\par
\ulnone Since the illustration dataset can't be publicly avalable (legal rights) we will not provide the drive link for the dataset here. \par
We shared a folder named "\b project_307923052_302636675" \b0 with barakhadad@mail.tau.ac.il (if you encounter any problems please contact us and we will respond immediately \par
noabarzilay11@gmail.com).\par
\b StarGAN framework-\par
\b0 In order to be able to run the code with the illustration dataset download go to:\par
the folder project_307923052_302636675 -> DATASET ->  StarGanDataset\par
And download the illustrations.zip file and extract it in the following path: \par
\cf1 your_path_to_our_project\cf0\\DeepLearningProject\\stargan\\data\\\cf1 Extract the folder here\cf0\par
\b MUNIT framework-\par
\b0 In order to be able to run the code with the illustration dataset download go to:\par
the folder project_307923052_302636675 -> DATASET ->  MUNITDataset\par
And download the illustrations2landscapes.zip file and extract it in the following path: \par
\cf1 your_path_to_our_project\cf0\\DeepLearningProject\\MUNIT\\datasets\\\cf1 Extract the folder here\cf0\par
3. \ul\b Training \ulnone\b0\par
in order to train the different genrators fro scratch-\par
\b StarGAN framework-\par
\b0 In the report we described several models, we are going to elaborate here how to train each model:\par
\ul MillGAN \ulnone -\par
python main.py --mode train --num_domains 2 --w_hpf 0 --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --\highlight2 vgg_w 1\par
\highlight0\ul Model A- Baseline StarGan - \par
\ulnone python main.py --mode train --num_domains 2 --w_hpf 0 --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --\highlight2 use_star_gen 1\par
\highlight0\ul Model B- \par
\ulnone python main.py --mode train --num_domains 2 --w_hpf 0 --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --\highlight2 use_residual_upsample 0\cf3\f1\fs24\par
\cf0\highlight0\ul\f0\fs22 Model D-\par
\ulnone python main.py --mode train --num_domains 2 --w_hpf 0 --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --\highlight2 loss_sacl 1\par
\highlight0\ul Model E\par
\ulnone python main.py --mode train --num_domains 2 --w_hpf 0 --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --\highlight2 vgg_w 1 \highlight0 --\highlight2 loss_sacl 1\highlight0\b\par
MUNIT framework-\par
\b0 In the report we described several models, we are going to elaborate here how to train each model:\par
\ul Model A - baseline-\par
\ulnone Go to "illustrations2landscapes_folder.yaml" in MUNIT\\configs-\par
Chane in the file the following-\par

\pard\box\brdrdash\brdrw0 \ltrpar\sa200\sl276\slmult1\cf4\f1\fs24 ganilla_gen\cf5 : False                      \cf6\i\par

\pard\ltrpar\sa200\sl276\slmult1\cf0\ul\i0\f0\fs22 Model B-\par
\ulnone Chane in the file the following-\par

\pard\box\brdrdash\brdrw0 \ltrpar\sa200\sl276\slmult1\cf4\f1\fs24 use_style_enc_simple\cf5 : True \par

\pard\ltrpar\sa200\sl276\slmult1\cf0\ul\f0\fs22 Model C-\par
\ulnone Chane in the file the following-\par

\pard\box\brdrdash\brdrw0 \ltrpar\sa200\sl276\slmult1\cf4\f1\fs24 use_style_enc_simple\cf5 : False \par

\pard\ltrpar\sa200\sl276\slmult1\cf0\ul\f0\fs22 Model E-\ulnone\par
Chane in the file the following-\par

\pard\box\brdrdash\brdrw0 \ltrpar\sa200\sl276\slmult1\cf4\f1\fs24 use_patch_gan\cf5 : False\par

\pard\ltrpar\sa200\sl276\slmult1\cf0\ul\f0\fs22 Model F- \ulnone\par
Chane in the file the following-\par

\pard\box\brdrdash\brdrw0 \ltrpar\sa200\sl276\slmult1\cf4\f1\fs24 style_dim\cf5 : 16\cf0\f0\fs22\par

\pard\ltrpar\sa200\sl276\slmult1 For all Run the following command -\par
\b python train.py --config configs/illustrations2landscapes_folder.yaml\par
\par
\b0 4. \ul\b Evaluating pretrained models -\par
\ulnone StarGAN framework-\b0\par
In order to download pretrained models go to:\par
the folder project_307923052_302636675 -> TrainedGenerators->  StarGan\par
Pick the pretrained model you wish to evaluate and enter the folder (for example "MillGAN"). \par
The dorralated pretrained models according to the report- \par
Model A - exprOriginal\par
Model B- exprGanilla_AdainNoResidualinup\par
Model C- exprGanilla_AdainResBlocksinUp\par
Model D- exprGanilla_AdainResBlocksinUp_SACL\par
Model E- exprGanilla_AdainResBlocksinUp_VGGContentLoss_SACL\par
MillGAN - it's simply MillGAN :)\par
All other models in the drive appears in the Appendix of the report. \par
\par
copy the content of the checkpoints folder from the model you pick to : \par
\cf1 your_path_to_our_project\cf0\\DeepLearningProject\\stargan\\expr\\\cf1 checkpoints\par
\cf0 Rum the following command -\par
\b\lang1033 IMPORTANT \b0 - You need to add the additional flags presented in the former section to the command line here as well (for example for evaluating the baselime model you shuld run the commands with \lang9 --use_star_gen 1\lang1033 )\lang9\par
\ul For Generate images: \par
\ulnone --mode eval --num_domains 2 --w_hpf 0 --resume_iter 100000 --train_img_dir data\\illustrations\\train --val_img_dir data\\illustrations\\val --checkpoint_dir expr\\checkpoints\\ --eval_dir expr\\eval\par
\ul For generate images from refences illustrations:\par
\ulnone Create a folder with the following path:\par
assets\\representative\\illustrations\\src  \par
Drop there all the \b natural \b0 images you wish to transfer to illustrations.\par
Create another folder with the folwing path:\par
assets\\representative\\illustrations\\ref\par
Drop there all of the \b illustration \b0 images you wish the generator will extract the style code from. \par
Run the following command and you will get an image simillar to Figure 9 from report:\par
--mode sample --num_domains 2 --resume_iter 100000 --w_hpf 0 --checkpoint_dir expr\\checkpoints\\ --result_dir expr\\results\\ --src_dir assets\\representative\\illustrations\\src --ref_dir assets\\representative\\illustrations\\ref\par
\cf1\par
\cf0\b MUNIT framework-\par
\b0 In order to download pretrained models go to:\par
the folder project_307923052_302636675 -> TrainedGenerators->  MUNIT\par
Download the models direcory and copy it to:\par
\cf1 your_path_to_our_project\cf0\\DeepLearningProject\\MUNIT\\\cf1 models\cf0\par
Model A - OriginalMUNIT\par
Model B- allup_adain_vgg_ncyc_styleEncMunit_total\par
Model C- allup_adain_vgg_cyc_total\par
Model D- allup_adain_vgg_ncyc_styleEncMunit5nd_total\par
Model E- allup_adain_vgg_ncyc_styleEncMunit_total_MSdis\par
ModelF - allup_adain_vgg_ncyc_styleEncMunit_16styledim_total\par
All other models in the drive appears in the Appendix of the report. \par
\b\lang1033 IMPORTANT \b0 - Each model you wish to test shole be tested with the proper configuration presented in the former section "Training", for example, Model A should contain in the \lang9 illustrations2landscapes_folder.yaml file "ganilla_gen:  False". \par
--config configs/illustrations2landscapes_folder.yaml --input inputs/\cf1 Input_image_you_chose \cf0 --output_folder results/model_results --checkpoint models/\cf1 Pre-trained_model_you_chose\cf0 .pt --a2b 0\par
\par
\b\par
\ul\par
}
 